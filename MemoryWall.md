# Il Muro della Memoria: Passato, Presente e Futuro delle DRAM

## L'Era d'Oro e il Declino della Legge di Moore per le Memorie

Mentre il mondo continua a interrogarsi sulla fine della Legge di Moore, la vera tragedia è che questa è già morta oltre un decennio fa, senza alcun clamore o titoli sui giornali. L'attenzione si concentra generalmente sulla logica dei processori, ma la Legge di Moore si è sempre applicata anche alle memorie DRAM, acronimo di Dynamic Random Access Memory, ovvero memoria dinamica ad accesso casuale.

Le DRAM hanno smesso di scalare efficacemente. Durante i giorni gloriosi del settore, la densità dei bit di memoria raddoppiava ogni diciotto mesi, superando persino la logica dei processori. Questo si traduceva in un aumento di densità di oltre cento volte ogni decennio. Tuttavia, nell'ultimo decennio la scalabilità è rallentata così drasticamente che la densità è aumentata solo del doppio in dieci anni, un rallentamento di circa cinquanta volte rispetto al ritmo storico.

Con l'esplosione dell'intelligenza artificiale, l'equilibrio dell'industria è stato sconvolto ulteriormente. Mentre i chip logici hanno migliorato significativamente sia la densità che il costo per funzione transistor nel tempo, i miglioramenti nelle velocità delle DRAM sono stati lenti. Nonostante la disinformazione diffusa, il costo per transistor continua a diminuire sui nodi da tre nanometri e due nanometri di TSMC (Taiwan Semiconductor Manufacturing Company, il principale produttore di semiconduttori al mondo). Per quanto riguarda la memoria, invece, l'aumento della larghezza di banda è ottenuto attraverso soluzioni di packaging eroiche e costose.

La memoria ad alta larghezza di banda, comunemente nota come HBM (High Bandwidth Memory), che costituisce la spina dorsale della memoria degli acceleratori per intelligenza artificiale, costa tre volte o più per gigabyte rispetto alla memoria DDR5 standard. I clienti sono costretti ad accettare questi costi poiché esistono poche alternative se vogliono creare un pacchetto acceleratore competitivo. Questo equilibrio è instabile: le future generazioni di HBM continuano a diventare ancora più complesse con un numero maggiore di strati impilati. Le esigenze di memoria per l'intelligenza artificiale stanno esplodendo, dato che i soli pesi dei modelli si avvicinano a scale di diversi terabyte. Per l'H100 di Nvidia, oltre il cinquanta percento del costo di produzione è attribuito all'HBM, e con Blackwell questa percentuale cresce fino a oltre il sessanta percento.

L'industria delle DRAM, in altre parole, ha colpito un muro invalicabile. I miglioramenti nel calcolo computazionale, sebbene in rallentamento, superano di gran lunga quelli della memoria. Come può il ritmo dell'innovazione riaccelerare nelle DRAM, e quali innovazioni possono essere sfruttate per migliorare larghezza di banda, capacità, costo ed efficienza energetica in futuro?

## La Gerarchia della Memoria e il Ruolo delle DRAM

Esistono diversi tipi di memoria utilizzati in un computer, ciascuno ottimizzato per scopi specifici. La più veloce è la SRAM (Static Random Access Memory, memoria statica ad accesso casuale), compatibile con le tecnologie dei processi logici e situata sulla CPU o GPU. Poiché si trova su un die logico, la SRAM è anche il tipo di memoria più costoso, circa cento volte più costosa per byte rispetto alla memoria dinamica ad accesso casuale, e viene quindi utilizzata solo in piccole quantità. All'estremità opposta dello spettro ci sono le memorie non volatili come le unità a stato solido NAND, i dischi rigidi e i nastri magnetici. Questi sono economici ma troppo lenti per molte attività. Le DRAM si collocano nella zona ideale tra SRAM e Flash: abbastanza veloci, abbastanza economiche.

Le DRAM possono rappresentare la metà del costo di un sistema server non dedicato all'intelligenza artificiale. Tuttavia, negli ultimi dieci anni sono state le più lente a scalare tra tutte le principali tecnologie di logica e memoria. I chip DRAM da sedici gigabit sono stati resi disponibili in grandi volumi otto anni fa ma sono ancora i più comuni oggi. Quando furono introdotti costavano circa tre dollari per gigabyte e raggiunsero un picco di quasi cinque dollari prima di tornare nell'intervallo dei tre dollari negli ultimi dodici mesi. Le velocità sono, semmai, leggermente più lente. Il consumo energetico ha visto i migliori miglioramenti, in gran parte grazie all'ascesa delle LPDDR (Low Power DDR), un cambiamento nel packaging che utilizza collegamenti più corti ed efficienti, ma l'asticella era bassa. La mancanza di progressi nella scalabilità delle DRAM rappresenta un collo di bottiglia prestazionale ed economico che frena la capacità di calcolo.

## Architettura di Base delle DRAM

In linea di principio, le DRAM sono semplici. Comprendono un array di celle di memoria disposte in una griglia, ciascuna che memorizza un bit di informazione. Tutte le DRAM moderne utilizzano una cella denominata 1T1C, che indica un transistor e un condensatore. Il transistor controlla l'accesso alla cella, mentre il condensatore memorizza l'informazione sotto forma di una piccola carica elettrica.

Le wordline, ovvero le linee di parola, collegano tutte le celle in una singola riga e controllano il transistor di accesso per ogni cella. Le bitline, ovvero le linee di bit, collegano tutte le celle in una singola colonna e si connettono alla sorgente del transistor di accesso. Quando una wordline viene energizzata, i transistor di accesso per tutte le celle nella riga si aprono e permettono il flusso di corrente dalla bitline nella cella durante la scrittura, o dalla cella alla bitline durante la lettura. Solo una wordline e una bitline saranno attive contemporaneamente, il che significa che solo la cella all'intersezione tra le linee attive verrà scritta o letta.

Le DRAM sono una tecnologia di memoria volatile: i condensatori di storage perdono carica e quindi richiedono frequenti refresh, aggiornamenti che possono avvenire anche ogni trentadue millisecondi circa, per mantenere i dati memorizzati. Ogni refresh legge il contenuto di una cella, aumenta la tensione sulla bitline a un livello ideale e lascia che quel valore aggiornato rifluisca nel condensatore. I refresh avvengono interamente all'interno del chip DRAM, senza che i dati fluiscano dentro o fuori dal chip. Questo minimizza lo spreco di energia, ma i refresh possono comunque arrivare a rappresentare oltre il dieci percento del consumo energetico totale delle DRAM.

I condensatori, proprio come i transistor, sono stati ridotti a dimensioni nanometriche ma anche con rapporti di aspetto estremi: circa mille nanometri di altezza ma solo decine di nanometri di diametro, con rapporti di aspetto che si avvicinano a cento a uno. La capacitanza è nell'ordine di sei-sette femtofarad. Ogni condensatore memorizza una carica estremamente piccola, circa quarantamila elettroni quando appena scritto.

La cella deve far entrare e uscire gli elettroni tramite la bitline, ma la tensione applicata alla bitline viene diluita da tutte le altre celle collegate alla stessa bitline. La capacitanza totale della bitline può superare i trenta femtofarad, una diluizione di cinque volte. La bitline è anche molto sottile, il che rallenta gli elettroni. Infine, la cella potrebbe essersi scaricata significativamente se non è stata aggiornata di recente, quindi ha solo una frazione della carica da fornire.

Tutti questi fattori significano che scaricare una cella per leggerne il valore può risultare in un segnale molto debole che deve essere amplificato. A questo scopo, gli amplificatori di senso, noti come sense amplifier, sono collegati alla fine di ogni bitline per rilevare le cariche estremamente piccole lette dalle celle di memoria e amplificare il segnale a una forza utile. Questi segnali più forti possono quindi essere letti altrove nel sistema come uno binario o zero.

L'amplificatore di senso ha un design circuitale ingegnoso: confronta la bitline attiva con una vicina corrispondente che non è in uso, iniziando con entrambe le linee portate a una tensione simile. La tensione sulla bitline attiva verrà confrontata con il vicino inattivo, sbilanciando l'amplificatore di senso e facendogli amplificare la differenza riportandola nella bitline attiva, sia amplificando il segnale che spingendo un nuovo valore completo, alto o basso, nella cella che rimane aperta alla bitline. È una situazione che prende due piccioni con una fava: la cella viene letta e aggiornata contemporaneamente.

## Storia delle DRAM: Quando Scalavano Ancora

Le DRAM moderne sono rese possibili da due invenzioni separate e complementari: la cella di memoria 1T1C e l'amplificatore di senso.

La cella 1T1C fu inventata nel 1967 alla IBM dal dottor Robert Dennard, noto anche per la sua omonima legge di scalabilità dei transistor MOS. Sia le DRAM che la scalabilità sono basate sui transistor MOS, che prendono il nome dagli strati nel gate del transistor: metallo, ossido e silicio.

Nonostante l'invenzione della struttura della cella di memoria 1T1C, le prime DRAM spedite da Intel nel 1973 utilizzavano tre transistor per cella, con il gate del transistor centrale che fungeva da condensatore di storage. Questa era una "gain cell", una cella con guadagno, dove il transistor centrale e quello finale fornivano guadagno per amplificare la carica molto piccola sul gate centrale, consentendo di leggere facilmente la cella senza disturbare il valore memorizzato.

Una cella 1T1C è migliore in teoria: meno dispositivi, più semplice da collegare e più piccola. Perché non fu adottata immediatamente? Non era ancora pratico leggere la cella. Al momento dell'invenzione, la piccola capacitanza della cella 1T1C la rendeva impraticabile da utilizzare. Era necessaria una seconda invenzione chiave: l'amplificatore di senso.

Il primo amplificatore di senso moderno fu sviluppato nel 1971 da Karl Stein alla Siemens, presentato a una conferenza in California e completamente trascurato. L'architettura 1T1C non era ampiamente adottata a quel punto e Siemens non aveva idea di cosa fare con questa invenzione. Stein fu spostato su un altro incarico dove ebbe una carriera di successo non correlata alle DRAM.

Questo design era ben abbinato alla spaziatura delle bitline ed è stato in grado di scalare più piccolo per tenere il passo con le dimensioni delle celle. L'amplificatore di senso è completamente spento quando non in uso, il che consente di averne milioni su un chip senza drenare energia. Sono stati un piccolo miracolo tecnologico.

Ci vollero più di cinque anni perché arrivasse il momento dell'amplificatore di senso. Robert Proebsting alla Mostek riscoprì indipendentemente il concetto e nel 1977 la loro DRAM da sedici kilobit con architettura 1T1C più amplificatore di senso divenne leader di mercato. Questa formula vincente rimase: l'architettura DRAM è fondamentalmente la stessa da quasi cinque decenni.

## L'Era d'Oro della Scalabilità

Nel ventesimo secolo, la Legge di Moore e la scalabilità di Dennard dominavano l'industria dei semiconduttori. Al suo apice, gli aumenti di densità delle DRAM superavano la logica. La capacità DRAM per chip raddoppiava ogni diciotto mesi, alimentando l'ascesa delle fabbriche giapponesi, che superarono per la prima volta la quota di mercato statunitense nel 1981 e raggiunsero il picco intorno all'ottanta percento nel 1987, e successivamente delle aziende coreane, la cui quota di mercato superò quella del Giappone nel 1998. La rapida sostituzione generazionale delle fabbriche su un processo relativamente semplice creò opportunità per nuovi entranti con i fondi per costruire la fabbrica di prossima generazione.

Il prezzo per bit si ridusse di tre ordini di grandezza in vent'anni in un'età d'oro della scalabilità delle DRAM. Questo ritmo non era sostenibile a lungo, e alla fine del ventesimo secolo e all'inizio del ventunesimo, la logica aveva superato significativamente la scalabilità della memoria. La recente scalabilità della logica è rallentata a un ritmo di miglioramenti di densità del trenta-quaranta percento ogni due anni. Ma questo è ancora buono rispetto alle DRAM che sono approssimativamente un ordine di grandezza più lente del loro picco, richiedendo ora dieci anni per un aumento di densità del doppio.

Questo rallentamento della scalabilità ha avuto effetti a catena nelle dinamiche dei prezzi delle DRAM. Mentre la memoria è stata tradizionalmente un'industria ciclica, la lenta scalabilità della densità ha significato una riduzione dei costi molto minore per ammortizzare gli aumenti di prezzo quando l'offerta è limitata. L'unico modo per aumentare l'offerta di DRAM è costruire nuove fabbriche. Le oscillazioni selvagge dei prezzi e gli alti investimenti in capitale significano che solo le aziende più grandi sopravvivono: più di venti produttori producevano DRAM a metà degli anni Novanta, con l'ottanta percento della quota di mercato distribuita tra i primi dieci. Ora i primi tre fornitori possiedono oltre il novantacinque percento del mercato.

Poiché le DRAM sono mercificate, i fornitori sono intrinsecamente molto più suscettibili alle fluttuazioni dei prezzi, a differenza della logica o dell'analogico, e devono competere principalmente sui prezzi grezzi dei loro prodotti quando il mercato è basso. La logica ha mantenuto la Legge di Moore solo con costi crescenti, le DRAM non hanno quel lusso. Il costo delle DRAM è semplice da misurare: dollari per gigabit. Rispetto ai periodi precedenti, gli ultimi dieci anni hanno visto una lenta diminuzione dei prezzi, solo un ordine di grandezza in un decennio quando prima ci voleva la metà di quel tempo.

## Le Sfide della Scalabilità Moderna

Da quando sono entrate nei nodi da dieci nanometri, la densità dei bit delle DRAM è stagnata. Anche l'aggiunta della litografia a ultravioletti estremi, nota come EUV, nei nodi 1z di Samsung e 1a di SK Hynix non ha aumentato significativamente la densità. Due sfide notevoli riguardano i condensatori e gli amplificatori di senso.

I condensatori sono difficili sotto molti aspetti. Innanzitutto, la modellazione è impegnativa poiché i fori devono essere strettamente impacchettati con un ottimo controllo della dimensione critica e della sovrapposizione, per contattare i transistor di accesso sottostanti ed evitare ponti o altri difetti. I condensatori hanno un rapporto di aspetto molto elevato e incidere un profilo di foro dritto e stretto è eccezionalmente difficile, ulteriormente complicato dalla necessità di una maschera dura più spessa per consentire un'incisione più profonda, poiché una maschera più spessa richiede un fotoresist più spesso che è più difficile da modellare.

Successivamente, devono essere depositati multipli strati privi di difetti di pochi nanometri di spessore sulle pareti lungo tutto il profilo del foro per formare il condensatore. Quasi ogni passaggio sforza i limiti della moderna tecnologia di elaborazione.

Gli amplificatori di senso sono una storia simile agli interconnessi logici. Una volta un ripensamento, ora sono di pari o addirittura maggiore difficoltà rispetto alle caratteristiche principali, ovvero i transistor logici e le celle di memoria. Sono compressi da più lati. La scalabilità dell'area deve essere eseguita per corrispondere alla riduzione delle bitline, con gli amplificatori di senso che diventano meno sensibili e più inclini a variazioni e perdite man mano che vengono resi più piccoli. Allo stesso tempo, i condensatori più piccoli immagazzinano meno carica, quindi il requisito di rilevamento per leggerli diventa più difficile.

Ci sono anche altre sfide, con il risultato che la scalabilità delle DRAM in modo economico è sempre più difficile utilizzando approcci tradizionali. La porta è aperta a nuove idee.

## Scalabilità a Breve Termine: Layout 4F² e Transistor a Canale Verticale

Nel breve termine, la scalabilità delle DRAM continuerà lungo la sua roadmap tradizionale. Cambiamenti più grandi e fondamentali all'architettura richiederanno anni per essere sviluppati e implementati. Nel frattempo, l'industria deve rispondere alla necessità di prestazioni migliori, anche se solo con miglioramenti marginali.

La roadmap a breve termine ha due innovazioni: il layout delle celle 4F² e i transistor a canale verticale, noti come VCT (Vertical Channel Transistor).

La notazione 4F² descrive l'area della cella di memoria in termini della dimensione minima della caratteristica F, simile alla metrica del track per l'altezza delle celle logiche standard. La dimensione minima della caratteristica è generalmente la larghezza della linea o dello spazio; nelle DRAM questa sarà la larghezza della wordline o della bitline. È un modo semplice per denotare la densità di un layout di celle e rende il confronto facile: una cella 4F² è solo i due terzi della dimensione di una cella 6F², offrendo un aumento teorico di densità del trenta percento senza scalare la dimensione minima della caratteristica. Si noti che il layout puro della cella non è l'unico limite alla scalabilità della densità, quindi i benefici reali sono probabilmente inferiori al caso ideale del trenta percento.

4F² è il limite teorico per una cella a bit singolo. Ricordiamo che la dimensione della caratteristica è la larghezza della linea o dello spazio, quindi un pattern linea più spazio avrà un pitch di 2F, non F, e quindi la dimensione minima possibile della cella è 4F² non solo F². Quindi, una volta raggiunta questa architettura, l'unica via per la scalabilità orizzontale è scalare F stesso, qualcosa che sta rapidamente diventando impraticabile, se non addirittura impossibile.

Le DRAM hanno utilizzato un layout 6F² dal 2007, con 8F² prima di allora. Un'eccezione notevole è CXMT, un fornitore cinese che ha utilizzato VCT e un layout 4F² nelle loro DRAM da diciotto nanometri dimostrate alla fine del 2023, progettate per aggirare le sanzioni. Poiché Samsung, SK Hynix e Micron sono stati in grado di scalare le celle, non sono stati costretti ad adottare queste architetture nello stesso modo in cui lo è stata CXMT. L'implicazione dell'adozione anticipata di CXMT è anche importante: è probabile che stiano avendo difficoltà a scalare F poiché hanno optato per il cambiamento più drastico nelle architetture di celle e transistor.

L'abilitatore chiave per le celle 4F² è il transistor a canale verticale. È necessario semplicemente perché il transistor deve scalare per adattarsi nella cella e entrambi i contatti, alla bitline e al condensatore, devono anche adattarsi in quell'ingombro, quindi una linea verticale. A queste scale diventa necessario costruire il transistor verticalmente invece che orizzontalmente, riducendo il suo ingombro fino a circa 1F, corrispondendo approssimativamente al condensatore sopra di esso, pur mantenendo una lunghezza di canale sufficiente affinché il transistor funzioni efficacemente.

Le attuali DRAM utilizzano canali orizzontali e source/drain con separazione orizzontale. Queste sono architetture mature e ben comprese. I VCT impilano sequenzialmente un source collegato alla bitline sotto di esso, un canale circondato dal gate e dalla wordline che controlla il gate, e un drain collegato al condensatore sopra. Ci sono compromessi nella fabbricazione dove alcuni passaggi diventano più facili e altri più difficili, ma nel complesso i VCT sono più difficili da produrre.

Il processo di Samsung è notevole per l'uso del bonding dei wafer, ovvero l'incollaggio di wafer. In un processo simile alla fornitura di alimentazione dal lato posteriore per la logica, i transistor di accesso alle celle vengono fabbricati con bitline formate sopra prima di capovolgere il wafer e incollarlo a un wafer di supporto, quindi la bitline è ora sepolta. Interessante notare che la base incollata non sembra richiedere un allineamento accurato con i VCT, sebbene la divulgazione non spieghi se la CMOS periferica sarà sul chip capovolto o nella base appena incollata. Il lato superiore viene assottigliato per esporre l'altra estremità dei transistor in modo che i condensatori di storage possano essere costruiti sopra di essi.

## Le Varianti Attuali di DRAM

Le DRAM sono disponibili in molte varietà, ciascuna ottimizzata per obiettivi diversi. Le varianti più recenti rilevanti sono DDR5, LPDDR5X, GDDR6X e HBM3/E. Le differenze tra loro risiedono quasi interamente nei circuiti periferici. Le celle di memoria stesse sono simili tra le varietà e i metodi di fabbricazione sono in generale simili per tutti i tipi.

La DDR5, dove DDR sta per Double Data Rate quinta generazione, offre la massima capacità di memoria poiché è confezionata in moduli di memoria dual in-line, noti come DIMM. La LPDDR5X, Low Power DDR5 con X che significa enhanced, ovvero migliorata, fornisce un funzionamento a basso consumo ma richiede distanze più brevi e connessioni a bassa capacitanza alla CPU che limitano la capacità, quindi viene utilizzata in telefoni cellulari e laptop dove il basso consumo è desiderabile e i vincoli di layout tollerabili.

Più recentemente abbiamo visto packaging a capacità maggiore per LPDDR utilizzato in alcuni acceleratori per l'intelligenza artificiale, nelle workstation professionali di Apple e nelle CPU feeder per AI come Grace. Questi nuovi usi sono guidati dalla ricerca di trasferimenti di dati efficienti dal punto di vista energetico e alta larghezza di banda.

Negli acceleratori, l'LPDDR è emersa come la migliore opzione per un secondo livello di memoria che fornisce capacità più economica a un livello inferiore e più lento rispetto alla costosa HBM. Non raggiunge le capacità massime e le caratteristiche di affidabilità, ma batte i DIMM DDR5 poiché consuma un ordine di grandezza meno energia per bit di throughput.

Il packaging LPDDR5X arriva fino a quattrocentottanta gigabyte disponibili sul processore Nvidia Grace, che è circa dieci volte il limite di capacità per le configurazioni GDDR, limitate dalle regole di layout del circuito stampato e del packaging dei chip necessari per soddisfare i requisiti di segnale nei sistemi di gioco consumer, e nella stessa gamma delle configurazioni server DDR medie. Una capacità DDR5 maggiore è possibile utilizzando R-DIMM di dimensioni superiori a centoventotto gigabyte, sebbene costosi a causa della complessità del packaging e dei chip Registers aggiuntivi, una sorta di chip buffer, sui DIMM.

L'LPDDR5X ha un grande vantaggio nel consumo energetico rispetto al DDR e nel costo rispetto all'HBM, ma l'energia per bit non può sfidare l'HBM e richiede molte lane, ovvero connessioni alla CPU, che affollano i layout delle schede a capacità maggiori. Ha anche una storia debole sulla correzione degli errori, nota come ECC (Error Correction Code), che diventa più importante a capacità maggiori poiché c'è una maggiore possibilità di errore. Per compensare, una certa capacità deve essere deviata per supportare ECC extra. Ad esempio, la CPU Grace ha cinquecentododici gigabyte di LPDDR5x per tray di calcolo ma sembra riservare trentadue gigabyte per funzionalità di affidabilità, lasciando quattrocentottanta gigabyte disponibili per l'uso.

Il prossimo standard LPDDR6 mostra pochi miglioramenti, mantenendo alti conteggi di lane per chip e aumenti di velocità relativamente moderati insieme a un supporto limitato per la correzione degli errori. L'LPDDR6 non fornirà un concorrente per l'HBM.

La GDDR6X, dove G sta per Graphics, si concentra sulle applicazioni grafiche, offrendo alta larghezza di banda a basso costo ma con latenza più alta e maggiore consumo energetico. Sebbene utile nelle GPU da gaming, è stata progettata con limiti di capacità a livello di scheda e livelli di potenza che limitano la dimensione delle applicazioni AI che possono utilizzarla.

Poi c'è l'HBM3E, High Bandwidth Memory terza generazione con una versione enhanced migliorata. Dà priorità alla larghezza di banda e all'efficienza energetica ma è molto costosa. Le due caratteristiche distintive dell'HBM sono la larghezza del bus molto più ampia e i die di memoria impilati verticalmente. I singoli die HBM hanno duecentocinquantasei bit ciascuno di input/output, sedici volte più dell'LPDDR che ha una larghezza del bus di soli sedici bit per chip. I die sono impilati verticalmente, tipicamente otto o più, con input/output raggruppati per ogni quattro die; in totale il package può fornire milleventiquattro bit di larghezza di banda. Nell'HBM4 questo raddoppierà a duemilaquarantotto bit. Per sfruttare al meglio l'HBM è meglio co-confezionarlo accanto al motore di calcolo per ridurre latenza ed energia per bit. Per espandere la capacità mantenendo una connessione breve al calcolo, devono essere aggiunti più die allo stack.

L'alto costo dell'HBM è principalmente guidato da questa necessità di impilamento dei die. In uno stack HBM tipico, otto o dodici die DRAM, con sedici e oltre sulla roadmap, sono impilati uno sopra l'altro, con alimentazione e segnale instradati da Through Silicon Via, abbreviati come TSV, in ogni die. I TSV sono collegamenti che passano direttamente attraverso il chip, che consentono la connessione tra chip. I TSV sono molto più densi, più performanti e più costosi dei vecchi metodi di wire-bonding, ovvero saldatura a filo, utilizzati per collegare chip impilati. Più di milleduecento fili di segnale devono essere instradati tramite TSV in uno stack HBM. Un'area significativa deve essere dedicata a loro, rendendo ogni die DRAM HBM il doppio delle dimensioni di un die DDR standard per la stessa capacità. Ciò significa anche requisiti di binning più elevati per le prestazioni elettriche e termiche del die DRAM.

Questa complessità riduce la resa. Ad esempio, gli errori di progettazione DRAM di Samsung e il loro uso di un nodo 1α in ritardo stanno contribuendo alle loro rese HBM scioccantemente scarse. Il packaging è l'altra sfida principale. Allineare correttamente otto o più die con migliaia di connessioni ciascuno è difficile e quindi costoso a causa di rese relativamente basse. Al momento questo è uno dei differenziatori chiave tra i fornitori di HBM, poiché SK Hynix può produrre con successo HBM3E con il loro packaging MR-MUF mentre Samsung fatica a ottenere resa dal loro prodotto. Micron ha una soluzione praticabile, ma deve scalare significativamente la produzione.

Nonostante gli alti costi e le sfide di resa, l'HBM3E è, per ora, il prodotto più prezioso e con il margine più alto che l'industria della memoria abbia mai avuto. Questo è principalmente perché per gli acceleratori AI di modelli grandi, nessun altro tipo di DRAM è un'alternativa praticabile. Mentre i margini probabilmente si eroderanno man mano che Samsung migliora la resa e Micron scala la produzione, l'appetito di memoria degli acceleratori AI continuerà a crescere, compensando in una certa misura il beneficio di questa nuova offerta.

In breve, l'alta larghezza di banda e la densità di packaging molto alta insieme alla migliore energia per bit e alla vera capacità ECC rendono l'HBM3E il chiaro vincitore, per ora, per gli acceleratori AI. Questo è il motivo per cui prodotti come l'H100 di Nvidia e l'MI300X di AMD lo utilizzano.

## La Roadmap dell'HBM e le Sfide Future

L'attuale soluzione HBM è costosa e sarà sempre più difficile da scalare. Come siamo finiti in questa situazione? L'HBM è una soluzione di packaging costruita attorno a idee DRAM legacy, ma confezionata con densità e adiacenza per cercare di risolvere i problemi di larghezza di banda e potenza per l'intelligenza artificiale e altre forme di calcolo ad alte prestazioni.

Tutte le GPU AI leader ora utilizzano HBM come memoria. I piani per il 2025 prevedono HBM3e a dodici strati con chip da trentadue gigabit per un totale di quarantotto gigabyte per stack, con velocità di dati fino a otto gigabit al secondo per filo. Nei server GPU le prime versioni di memoria unificata con una CPU di supporto sono state lanciate con l'MI300A di AMD e il Grace Hopper di Nvidia.

La CPU Grace ha LPDDR5X ad alta capacità, mentre la GPU ha HBM3 ad alta larghezza di banda. Tuttavia, la CPU e la GPU sono su package separati, collegati tramite NVLink-C2C a novecento gigabyte al secondo. Questo modello è più semplice da integrare ma più difficile sul lato software. La latenza della memoria collegata all'altro chip è molto più alta e potrebbe influenzare un numero significativo di workload. Come tale, la memoria non è del tutto uniforme e presenta le proprie sfide.

## L'HBM4 e le Innovazioni Future

L'HBM4 è a pochi anni di distanza, con Samsung e Micron che affermano che arriverà fino a sedici strati con un throughput di un virgola cinque terabyte al secondo per stack. Questo è più del doppio della larghezza di banda di quello che abbiamo oggi con solo un fattore da uno virgola tre a uno virgola cinque volte il consumo energetico, ma questa scalabilità non è sufficiente, poiché il consumo energetico della memoria continua ad aumentare complessivamente. L'HBM4 cambierà anche a una larghezza di duemilaquarantotto bit per stack, riducendo le velocità di dati di una piccola quantità a sette virgola cinque gigabit al secondo, aiutando con il consumo energetico e l'integrità del segnale. È probabile che le velocità di dati aumenteranno ai livelli dell'HBM3E con l'HBM4E o qualcosa di simile.

L'altro cambiamento significativo riguarda il die base dell'HBM. Il die base verrà fabbricato su processi FinFET anziché sulla tecnologia CMOS planare utilizzata ora. Per Micron e SK Hynix che non hanno questa capacità logica, il die base verrà fabbricato da una foundry, una fabbrica dedicata ai semiconduttori, con TSMC che ha già fatto annunci che sarà il partner per SK Hynix. Inoltre, ci sarà una personalizzazione del die base per i singoli clienti.

Gli annunci sull'HBM4 prevedono che almeno due diverse forme di chip base saranno in uso, permettendo di ottimizzare l'interfaccia di memoria per diverse velocità e lunghezze. È probabile che la funzionalità che controlla la macchina a stati della DRAM si sposterà sul chip base per controllare più efficientemente i chip DRAM, e le connessioni solo verticali potrebbero consentire di ridurre l'energia per bit.

L'HBM personalizzato può abilitare molteplici altre architetture di package al di fuori degli assemblaggi convenzionali basati su CoWoS, ovvero Chip on Wafer on Substrate, un tipo di packaging avanzato, che vediamo oggi. Potrebbe esserci un repeater PHY, ovvero un ripetitore di interfaccia fisica, per concatenare in serie più righe di HBM, anche se qualsiasi cosa oltre i due ranghi vedrebbe rendimenti decrescenti.

Con l'HBM4 e i successori, è stato suggerito il passaggio al bonding ibrido, ovvero l'incollaggio ibrido. Questo permetterà stack HBM più sottili poiché il gap dei bump, le piccole sfere di saldatura, viene rimosso, e una migliore dissipazione del calore. Inoltre, permetterà altezze di stack da sedici a venti o più strati. Potrebbe anche ridurre il consumo energetico di una piccola quantità poiché la distanza fisica che i segnali percorrono sarà ridotta. Le sfide sono sostanziali però: ottenere la resa di uno stack incollato di sedici o più die, nessuno perfettamente piatto, non è facile. Nessuno è vicino a una soluzione pronta per la produzione ad alto volume qui.

Tutto l'HBM4 iniziale non utilizzerà il bonding ibrido, e ci aspettiamo che questo rimanga vero per molto più tempo di quanto la maggior parte spererebbe.

## Interfacce Personalizzate e Nuove Architetture

La connessione tra CPU, GPU o acceleratore e memoria si trova nel chip base. Migliorare questa connessione è una possibile via per superare le limitazioni della memoria. Eliyan, una startup finanziata da Micron e Intel tra gli altri, sta guidando questo approccio con la loro interfaccia personalizzata UMI, che sta per Universal Memory Interconnect.

Questa interfaccia UMI viene utilizzata con un die ASIC, ovvero Application-Specific Integrated Circuit, un circuito integrato specifico per l'applicazione, che funge da chip base per uno stack HBM o da controller di modulo per altri tipi di memoria. Questo chiplet contiene sia il controller di memoria che l'interconnessione fisica alla memoria, nota come PHY. L'UMI si connette esternamente alla GPU host, collegandosi al fabric, ovvero l'infrastruttura di interconnessione, dell'host.

Prodotti con un processo CMOS completo, possono essere veloci ed efficienti, utilizzando un protocollo avanzato chiamato Nulink per connettersi all'host ed eliminare l'ingombro del controller di memoria dal silicio host.

Le tecnologie di packaging di Eliyan funzionano anche con substrati standard e hanno una portata molto maggiore rispetto al packaging avanzato regolare. Questo potrebbe consentire HBM che non è adiacente al die ASIC, ma molto più lontano, il che significa che capacità più elevate possono essere accomodate. Il loro approccio utilizza anche meno area e meno bordo sull'host, il che significa che la larghezza del canale può essere aumentata. I chiplet di memoria UMI standardizzati potrebbero consentire di utilizzare HBM, DDR, memoria CXL e altro senza essere fissati a un tipo specifico, aumentando significativamente la flessibilità. Sebbene sia possibile che questo approccio possa offrire miglioramenti a breve termine, non affronta i problemi di costo sottostanti con l'HBM.

## Memorie Emergenti: Oltre le DRAM Tradizionali

Per tutto il tempo in cui DRAM e NAND sono state dominanti, c'è stata ricerca su alternative migliori. Il termine ombrello per queste è "memorie emergenti". È un po' un termine improprio poiché, finora, nessuna di esse è riuscita a emergere in un prodotto ad alto volume. Date le nuove sfide e gli incentivi legati all'intelligenza artificiale, vale comunque la pena di una breve discussione.

La memoria più promettente per applicazioni discrete è la FeRAM, ovvero Ferroelectric RAM, memoria ferroelettrica ad accesso casuale. Invece di utilizzare un dielettrico, ovvero un materiale isolante, nel condensatore di storage, usano un ferroelettrico, un materiale che si polarizza in un campo elettrico. Questi hanno la caratteristica desiderabile di essere non volatili, cioè possono memorizzare dati quando sono spenti e non sprecano energia o tempo nei refresh.

Micron ha mostrato risultati promettenti all'IEDM, International Electron Devices Meeting, una conferenza internazionale sui dispositivi elettronici, nel 2023 con densità comparabile alla loro DRAM D1β insieme a buone prestazioni di endurance, ovvero resistenza ai cicli di scrittura, e retention, ovvero ritenzione dei dati. In altre parole un buon candidato per l'uso in AI e machine learning se non fosse per un problema: il costo. È complessa da produrre e fa più uso di materiali esotici rispetto alle DRAM convenzionali, al punto che semplicemente non è competitiva al momento.

La MRAM, Magnetic RAM, memoria magnetica ad accesso casuale, è un'altra area di ricerca promettente. Invece di utilizzare cariche elettriche, i dati vengono memorizzati tramite mezzi magnetici. La maggior parte dei design utilizza giunzioni a tunnel magnetico, note come MTJ (Magnetic Tunnel Junction), come cella di storage dei bit.

All'IEDM 2022, SK Hynix e Kioxia hanno mostrato una cella MTJ con un selettore con un pitch di quarantacinque nanometri e una dimensione critica di venti nanometri. Insieme, hanno raggiunto la più alta densità MRAM fino ad oggi di zero virgola quarantanove gigabit per millimetro quadrato, maggiore della DRAM D1β di Micron che ha una densità di zero virgola quattrocentotrentacinque gigabit per millimetro quadrato. La cella presenta persino un design 4F². Il loro obiettivo è produrre in package discreti come alternativa alle DRAM.

Al momento nessuna delle memorie alternative è ben posizionata per sfidare le DRAM. Alcune hanno celle più grandi o più lente. Alcune hanno processi più costosi. La maggior parte ha endurance limitata. Alcune hanno bassa resa. In pratica, i prodotti che vengono spediti per memorie magnetiche o a cambiamento di fase hanno dimensioni in megabyte, non gigabyte. Questo potrebbe cambiare, c'è molto denaro in gioco e una combinazione vincente potrebbe esistere in modo nascosto, ma c'è molto lavoro sia sui dispositivi che sulla scala di produzione da fare.

## Compute in Memory: Ripensare l'Architettura Fondamentale

Le DRAM sono state ostacolate fin dall'inizio dalla loro architettura. Sono una semplice macchina a stati senza alcuna logica di controllo, il che aiuta a mantenere basso il costo, ma significa che dipendono dall'host, la CPU, per controllarle.

Questo paradigma è saldamente radicato: i moderni processi di fabbricazione delle DRAM sono così pesantemente ottimizzati e specializzati che non possono realisticamente produrre logica di controllo. Il gruppo industriale JEDEC, Joint Electron Devices Engineering Council, che stabilisce gli standard per i componenti elettronici, impone anche intrusioni minime dalla logica quando sviluppa nuovi standard.

Il chip DRAM è totalmente dipendente dall'host: tutti i comandi vengono incanalati attraverso un'interfaccia condivisa per più banchi nella memoria, per conto di più thread nell'host. Ogni comando richiede quattro o più passaggi da emettere con tempi precisi per mantenere la DRAM funzionante correttamente. I chip DRAM non hanno nemmeno la logica per evitare conflitti.

Questo è esacerbato dall'uso di un'interfaccia half-duplex antica: un chip DRAM può leggere o scrivere dati ma non entrambi contemporaneamente. L'host ha un modello esatto della DRAM e deve prevedere se l'interfaccia dovrebbe essere impostata per leggere o scrivere per ogni ciclo di clock. I comandi e i dati vengono inviati su fili separati, il che riduce la complessità temporale ma aumenta il conteggio dei fili e l'affollamento della spiaggia, ovvero lo spazio disponibile per le connessioni sulla GPU o CPU. Nel complesso, l'interfaccia di memoria è scesa di un ordine di grandezza al di sotto dei bit rate, della densità della spiaggia e dell'efficienza delle PHY alternative utilizzate dai chip logici.

Il risultato di questi svantaggi è che i DIMM DDR5, i più comuni sui server, spendono oltre il novantanove percento dell'energia di lettura o scrittura nel controller host e nell'interfaccia. Altre varianti sono leggermente migliori: l'uso energetico dell'HBM è approssimativamente novantacinque percento interfaccia, cinque percento lettura/scrittura della cella di memoria, ma comunque da nessuna parte vicino al pieno potenziale delle DRAM.

La funzionalità è semplicemente nel posto sbagliato. Naturalmente, la soluzione è spostarla in quello corretto: la logica di controllo dovrebbe essere on-chip con la memoria. Questo è il Compute in Memory, abbreviato come CIM, ovvero calcolo nella memoria.

## Liberare il Potenziale dei Banchi di Memoria

I banchi DRAM hanno un potenziale di prestazioni incredibile che va quasi completamente sprecato a causa delle interfacce.

I banchi sono l'unità di base della costruzione DRAM. Comprendono otto sotto-banchi ciascuno con sessantaquattro megabit, ovvero ottomila righe per ottomila bit, di memoria. Il banco attiva e aggiorna una riga di ottomila bit alla volta ma ne trasferisce solo duecentocinquantasei dentro o fuori in qualsiasi operazione di input/output. Questa limitazione è dovuta alle connessioni esterne dagli amplificatori di senso: mentre la riga è supportata da ottomila amplificatori di senso, solo uno su trentadue amplificatori di senso, ovvero duecentocinquantasei, è connesso fuori dal sotto-banco, il che significa che le operazioni di lettura o scrittura sono limitate a duecentocinquantasei bit.

Gli amplificatori di senso si trovano in un canyon circondato da alti condensatori. In uno smontaggio con fascio ionico focalizzato, noto come FIB (Focused Ion Beam), dell'ETH di Zurigo si può vedere che c'è cablaggio a livelli superiori che necessita di alti via, collegamenti verticali, che si estendono verso il basso per fare contatti agli amplificatori di senso.

Anche con questa interfaccia limitata, uno su trentadue accessibile in qualsiasi momento, la capacità di picco di lettura/scrittura di un banco è approssimativamente duecentocinquantasei gigabit al secondo, con una media più vicina a centoventotto gigabit al secondo poiché almeno il cinquanta percento del tempo viene utilizzato per passare a una nuova riga attiva. Con trentadue banchi per chip da sedici gigabit, il pieno potenziale di un chip è quattro terabyte al secondo.

Più in alto nella gerarchia, i banchi sono connessi in gruppi di banchi, che a loro volta si connettono all'interfaccia fuori dal chip DRAM. Nell'HBM, ogni die ha duecentocinquantasei linee dati con un throughput di picco di duecentocinquantasei gigabyte al secondo per die. Questo collo di bottiglia può utilizzare solo un sedicesimo del potenziale sottostante dei banchi.

Per aggiungere insulto all'ingiuria, sono necessari due picojoule di energia per trasferire un singolo bit fuori dal chip, venti volte più di quanto ci è voluto per spostarlo dentro o fuori dalla cella. La maggior parte di questo avviene alle due interfacce a ciascuna estremità dei fili DQ, Data Question-mark, una linea dati utilizzata sia per la lettura che per la scrittura, e nella logica del controller sull'host.

Con un'architettura così dispendiosa, è inevitabile che vengano fatti sforzi per accedere a più del potenziale di prestazioni.

## Il Percorso Verso il Futuro: Il Pieno Potenziale delle DRAM

Anche semplici esempi teorici mostrano che c'è un enorme potenziale disponibile qui. L'implementazione dello standard UCIe, Universal Chiplet Interconnect Express, uno standard per l'interconnessione tra chiplet, consentirebbe un throughput di undici terabit al secondo per millimetro di bordo, quasi dodici volte migliore dell'HBM3E. L'energia per bit scenderebbe di un ordine di grandezza da due picojoule a zero virgola venticinque picojoule. E l'UCIe non è nemmeno la soluzione più recente: lo standard proprietario Nulink di Eliyan, per fare solo un esempio, rivendica miglioramenti ancora maggiori.

L'avvertenza qui è che se il fabric host viene esteso attraverso l'interfaccia, allora un sottoinsieme del set di comandi del fabric deve essere gestito sul lato DRAM. Ogni banco avrebbe bisogno di implementare localmente la macchina a stati: pre-carica, selezione dell'indirizzo, attivazione, lettura/scrittura, chiusura, eccetera. Questo richiede logica relativamente complessa fabbricata on-chip con la DRAM.

Aggiungere logica a un chip DRAM non è, ovviamente, un compito semplice. La buona notizia è che l'HBM include un chip base CMOS, e quando arriverà la DRAM 3D c'è una certezza virtuale che buona logica CMOS venga incollata sopra o sotto lo stack di memoria. In altre parole, l'architettura è favorevole all'inclusione di un certo calcolo all'interno della memoria, e i produttori di chip saranno incentivati a farlo.

## Il Percorso Avanti e i Possibili Vincitori

C'è frutta facile da cogliere qui: considerate cosa si potrebbe fare se l'HBM adottasse la velocità della GDDR7 di trentadue gigabit al secondo per filo dati. La GDDR7 dimostra che transistor abbastanza veloci possono essere fatti sui chip DRAM, e la distanza verticale attraverso i TSV verso lo stack base è sotto un millimetro, il che dovrebbe mantenere l'energia per bit nell'intervallo di zero virgola venticinque picojoule per bit. Sorge la domanda: perché JEDEC non si orienterebbe verso uno standard migliorato qui?

Le interfacce esterne sul chip base potrebbero essere sostanzialmente aggiornate a design moderni che offrono più di un terabyte al secondo per millimetro di bordo, a energia frazionale di picojoule per bit. Qualcuno vincerà alla grande nelle guerre della proprietà intellettuale. Sebbene sia possibile che JEDEC adotti una scelta come standard, più probabilmente sarà fatto da coppie memoria-fornitore GPU che si muovono più velocemente, poiché JEDEC di solito impiega anni.

Vediamo già un cambiamento reale possibile nell'HBM4 con l'accettazione di chip base di terze parti, che è destinato a scatenare esperimenti. Vedremo probabilmente controllo del canale scaricato, pura estensione del fabric sull'interconnessione, ridotta energia per bit su centimetri di distanza e concatenamento in serie ad altre righe di HBM più lontane dall'host, o a memoria di secondo livello come banchi di LPDDR.

In questo modo i design possono aggirare i limiti di potenza nel tentativo di fare calcolo all'interno dello stack di memoria e invece utilizzare un'interfaccia modernizzata sul chip base per consentire ai chip vicini la larghezza di banda e la bassa energia per bit per il calcolo come se fosse nella memoria.

## Conclusioni e Prospettive

L'industria delle DRAM si trova a un punto di svolta critico. Dopo decenni di progressi costanti, la scalabilità tradizionale ha raggiunto limiti fondamentali. I condensatori sono così piccoli e con aspetti così estremi che ulteriori riduzioni sono quasi impossibili con le tecnologie attuali. Gli amplificatori di senso devono rilevare cariche sempre più deboli mentre vengono compressi in spazi sempre più piccoli.

L'esplosione dell'intelligenza artificiale ha reso questi problemi ancora più urgenti. I modelli di AI richiedono quantità massicce di memoria con larghezza di banda estremamente elevata, e l'HBM, sebbene efficace, è costosa e difficile da produrre. Con l'HBM che rappresenta oltre il sessanta percento del costo dei sistemi come Blackwell di Nvidia, l'industria non può permettersi di continuare su questa traiettoria indefinitamente.

Le soluzioni a breve termine, come i layout 4F² e i transistor a canale verticale, offrono miglioramenti incrementali ma non risolvono i problemi fondamentali. Le memorie emergenti come FeRAM e MRAM mostrano promesse ma devono ancora superare sfide significative di costo, endurance e scala di produzione prima di poter sfidare seriamente le DRAM consolidate.

La vera rivoluzione probabilmente verrà dall'architettura Compute in Memory e dall'arrivo della DRAM 3D. Spostare la logica di controllo all'interno dei chip di memoria, utilizzare interfacce moderne ad alta velocità e sfruttare l'enorme potenziale latente dei banchi di memoria potrebbe sbloccare miglioramenti di prestazioni di ordini di grandezza. L'energia per bit potrebbe scendere da due picojoule a zero virgola venticinque picojoule, mentre il throughput potrebbe aumentare di dieci volte o più.

L'HBM4 con chip base personalizzati rappresenta il primo passo verso questa nuova era. L'accettazione di chip base di terze parti e la possibilità di personalizzazione per singoli clienti apriranno la porta a innovazioni che finora erano impossibili sotto gli standard rigidi di JEDEC.

I vincitori in questa transizione saranno probabilmente le aziende che possono integrare logica avanzata con memoria ad alte prestazioni: SK Hynix con la sua partnership con TSMC, e potenzialmente Micron se riuscirà a scalare rapidamente la produzione. Samsung, nonostante le sue difficoltà attuali con le rese HBM, ha risorse enormi e potrebbe recuperare. Le startup come Eliyan che sviluppano interfacce innovative potrebbero diventare fornitori critici di proprietà intellettuale.

L'industria dei semiconduttori sta per entrare in un periodo di cambiamento fondamentale nell'architettura della memoria. Dopo cinquant'anni di relativa stabilità nell'architettura di base delle DRAM, la combinazione di limiti fisici e le esigenze dell'intelligenza artificiale stanno forzando una reimaginazione completa di come la memoria dovrebbe essere progettata, fabbricata e integrata nei sistemi. Le aziende che riusciranno a navigare con successo questa transizione domineranno l'era dell'intelligenza artificiale che si sta rapidamente avvicinando.
