# Il Muro della Memoria: Passato, Presente e Futuro delle DRAM

## L'Era d'Oro e il Declino della Legge di Moore per le Memorie

Mentre il mondo continua a interrogarsi sulla fine della Legge di Moore, la vera tragedia è che questa è già morta oltre un decennio fa, senza alcun clamore o titoli sui giornali. L'attenzione si concentra generalmente sulla logica dei processori, ma la Legge di Moore si è sempre applicata anche alle memorie DRAM, acronimo di Dynamic Random Access Memory, ovvero memoria dinamica ad accesso casuale.

Le DRAM hanno smesso di scalare efficacemente. Durante i giorni gloriosi del settore, la densità dei bit di memoria raddoppiava ogni diciotto mesi, superando persino la logica dei processori. Questo si traduceva in un aumento di densità di oltre cento volte ogni decennio. Tuttavia, nell'ultimo decennio la scalabilità è rallentata così drasticamente che la densità è aumentata solo del doppio in dieci anni, un rallentamento di circa cinquanta volte rispetto al ritmo storico.

Con l'esplosione dell'intelligenza artificiale, l'equilibrio dell'industria è stato sconvolto ulteriormente. Mentre i chip logici hanno migliorato significativamente sia la densità che il costo per funzione transistor nel tempo, i miglioramenti nelle velocità delle DRAM sono stati lenti. Nonostante la disinformazione diffusa, il costo per transistor continua a diminuire sui nodi da tre nanometri e due nanometri di TSMC (Taiwan Semiconductor Manufacturing Company, il principale produttore di semiconduttori al mondo). Per quanto riguarda la memoria, invece, l'aumento della larghezza di banda è ottenuto attraverso soluzioni di packaging eroiche e costose.

La memoria ad alta larghezza di banda, comunemente nota come HBM (High Bandwidth Memory), che costituisce la spina dorsale della memoria degli acceleratori per intelligenza artificiale, costa tre volte o più per gigabyte rispetto alla memoria DDR5 standard. I clienti sono costretti ad accettare questi costi poiché esistono poche alternative se vogliono creare un pacchetto acceleratore competitivo. Questo equilibrio è instabile: le future generazioni di HBM continuano a diventare ancora più complesse con un numero maggiore di strati impilati. Le esigenze di memoria per l'intelligenza artificiale stanno esplodendo, dato che i soli pesi dei modelli si avvicinano a scale di diversi terabyte. Per l'H100 di Nvidia, oltre il cinquanta percento del costo di produzione è attribuito all'HBM, e con Blackwell questa percentuale cresce fino a oltre il sessanta percento.

L'industria delle DRAM, in altre parole, ha colpito un muro invalicabile. I miglioramenti nel calcolo computazionale, sebbene in rallentamento, superano di gran lunga quelli della memoria. Come può il ritmo dell'innovazione riaccelerare nelle DRAM, e quali innovazioni possono essere sfruttate per migliorare larghezza di banda, capacità, costo ed efficienza energetica in futuro?

## La Gerarchia della Memoria e il Ruolo delle DRAM

Esistono diversi tipi di memoria utilizzati in un computer, ciascuno ottimizzato per scopi specifici. La più veloce è la SRAM (Static Random Access Memory, memoria statica ad accesso casuale), compatibile con le tecnologie dei processi logici e situata sulla CPU o GPU. Poiché si trova su un die logico, la SRAM è anche il tipo di memoria più costoso, circa cento volte più costosa per byte rispetto alla memoria dinamica ad accesso casuale, e viene quindi utilizzata solo in piccole quantità. All'estremità opposta dello spettro ci sono le memorie non volatili come le unità a stato solido NAND, i dischi rigidi e i nastri magnetici. Questi sono economici ma troppo lenti per molte attività. Le DRAM si collocano nella zona ideale tra SRAM e Flash: abbastanza veloci, abbastanza economiche.

Le DRAM possono rappresentare la metà del costo di un sistema server non dedicato all'intelligenza artificiale. Tuttavia, negli ultimi dieci anni sono state le più lente a scalare tra tutte le principali tecnologie di logica e memoria. I chip DRAM da sedici gigabit sono stati resi disponibili in grandi volumi otto anni fa ma sono ancora i più comuni oggi. Quando furono introdotti costavano circa tre dollari per gigabyte e raggiunsero un picco di quasi cinque dollari prima di tornare nell'intervallo dei tre dollari negli ultimi dodici mesi. Le velocità sono, semmai, leggermente più lente. Il consumo energetico ha visto i migliori miglioramenti, in gran parte grazie all'ascesa delle LPDDR (Low Power DDR), un cambiamento nel packaging che utilizza collegamenti più corti ed efficienti, ma l'asticella era bassa. La mancanza di progressi nella scalabilità delle DRAM rappresenta un collo di bottiglia prestazionale ed economico che frena la capacità di calcolo.

## Architettura di Base delle DRAM

In linea di principio, le DRAM sono semplici. Comprendono un array di celle di memoria disposte in una griglia, ciascuna che memorizza un bit di informazione. Tutte le DRAM moderne utilizzano una cella denominata 1T1C, che indica un transistor e un condensatore. Il transistor controlla l'accesso alla cella, mentre il condensatore memorizza l'informazione sotto forma di una piccola carica elettrica.

Le wordline, ovvero le linee di parola, collegano tutte le celle in una singola riga e controllano il transistor di accesso per ogni cella. Le bitline, ovvero le linee di bit, collegano tutte le celle in una singola colonna e si connettono alla sorgente del transistor di accesso. Quando una wordline viene energizzata, i transistor di accesso per tutte le celle nella riga si aprono e permettono il flusso di corrente dalla bitline nella cella durante la scrittura, o dalla cella alla bitline durante la lettura. Solo una wordline e una bitline saranno attive contemporaneamente, il che significa che solo la cella all'intersezione tra le linee attive verrà scritta o letta.

Le DRAM sono una tecnologia di memoria volatile: i condensatori di storage perdono carica e quindi richiedono frequenti refresh, aggiornamenti che possono avvenire anche ogni trentadue millisecondi circa, per mantenere i dati memorizzati. Ogni refresh legge il contenuto di una cella, aumenta la tensione sulla bitline a un livello ideale e lascia che quel valore aggiornato rifluisca nel condensatore. I refresh avvengono interamente all'interno del chip DRAM, senza che i dati fluiscano dentro o fuori dal chip. Questo minimizza lo spreco di energia, ma i refresh possono comunque arrivare a rappresentare oltre il dieci percento del consumo energetico totale delle DRAM.

I condensatori, proprio come i transistor, sono stati ridotti a dimensioni nanometriche ma anche con rapporti di aspetto estremi: circa mille nanometri di altezza ma solo decine di nanometri di diametro, con rapporti di aspetto che si avvicinano a cento a uno. La capacitanza è nell'ordine di sei-sette femtofarad. Ogni condensatore memorizza una carica estremamente piccola, circa quarantamila elettroni quando appena scritto.

La cella deve far entrare e uscire gli elettroni tramite la bitline, ma la tensione applicata alla bitline viene diluita da tutte le altre celle collegate alla stessa bitline. La capacitanza totale della bitline può superare i trenta femtofarad, una diluizione di cinque volte. La bitline è anche molto sottile, il che rallenta gli elettroni. Infine, la cella potrebbe essersi scaricata significativamente se non è stata aggiornata di recente, quindi ha solo una frazione della carica da fornire.

Tutti questi fattori significano che scaricare una cella per leggerne il valore può risultare in un segnale molto debole che deve essere amplificato. A questo scopo, gli amplificatori di senso, noti come sense amplifier, sono collegati alla fine di ogni bitline per rilevare le cariche estremamente piccole lette dalle celle di memoria e amplificare il segnale a una forza utile. Questi segnali più forti possono quindi essere letti altrove nel sistema come uno binario o zero.

L'amplificatore di senso ha un design circuitale ingegnoso: confronta la bitline attiva con una vicina corrispondente che non è in uso, iniziando con entrambe le linee portate a una tensione simile. La tensione sulla bitline attiva verrà confrontata con il vicino inattivo, sbilanciando l'amplificatore di senso e facendogli amplificare la differenza riportandola nella bitline attiva, sia amplificando il segnale che spingendo un nuovo valore completo, alto o basso, nella cella che rimane aperta alla bitline. È una situazione che prende due piccioni con una fava: la cella viene letta e aggiornata contemporaneamente.

## Storia delle DRAM: Quando Scalavano Ancora

Le DRAM moderne sono rese possibili da due invenzioni separate e complementari: la cella di memoria 1T1C e l'amplificatore di senso.

La cella 1T1C fu inventata nel 1967 alla IBM dal dottor Robert Dennard, noto anche per la sua omonima legge di scalabilità dei transistor MOS. Sia le DRAM che la scalabilità sono basate sui transistor MOS, che prendono il nome dagli strati nel gate del transistor: metallo, ossido e silicio.

Nonostante l'invenzione della struttura della cella di memoria 1T1C, le prime DRAM spedite da Intel nel 1973 utilizzavano tre transistor per cella, con il gate del transistor centrale che fungeva da condensatore di storage. Questa era una "gain cell", una cella con guadagno, dove il transistor centrale e quello finale fornivano guadagno per amplificare la carica molto piccola sul gate centrale, consentendo di leggere facilmente la cella senza disturbare il valore memorizzato.

Una cella 1T1C è migliore in teoria: meno dispositivi, più semplice da collegare e più piccola. Perché non fu adottata immediatamente? Non era ancora pratico leggere la cella. Al momento dell'invenzione, la piccola capacitanza della cella 1T1C la rendeva impraticabile da utilizzare. Era necessaria una seconda invenzione chiave: l'amplificatore di senso.

Il primo amplificatore di senso moderno fu sviluppato nel 1971 da Karl Stein alla Siemens, presentato a una conferenza in California e completamente trascurato. L'architettura 1T1C non era ampiamente adottata a quel punto e Siemens non aveva idea di cosa fare con questa invenzione. Stein fu spostato su un altro incarico dove ebbe una carriera di successo non correlata alle DRAM.

Questo design era ben abbinato alla spaziatura delle bitline ed è stato in grado di scalare più piccolo per tenere il passo con le dimensioni delle celle. L'amplificatore di senso è completamente spento quando non in uso, il che consente di averne milioni su un chip senza drenare energia. Sono stati un piccolo miracolo tecnologico.

Ci vollero più di cinque anni perché arrivasse il momento dell'amplificatore di senso. Robert Proebsting alla Mostek riscoprì indipendentemente il concetto e nel 1977 la loro DRAM da sedici kilobit con architettura 1T1C più amplificatore di senso divenne leader di mercato. Questa formula vincente rimase: l'architettura DRAM è fondamentalmente la stessa da quasi cinque decenni.

## L'Era d'Oro della Scalabilità

Nel ventesimo secolo, la Legge di Moore e la scalabilità di Dennard dominavano l'industria dei semiconduttori. Al suo apice, gli aumenti di densità delle DRAM superavano la logica. La capacità DRAM per chip raddoppiava ogni diciotto mesi, alimentando l'ascesa delle fabbriche giapponesi, che superarono per la prima volta la quota di mercato statunitense nel 1981 e raggiunsero il picco intorno all'ottanta percento nel 1987, e successivamente delle aziende coreane, la cui quota di mercato superò quella del Giappone nel 1998. La rapida sostituzione generazionale delle fabbriche su un processo relativamente semplice creò opportunità per nuovi entranti con i fondi per costruire la fabbrica di prossima generazione.

Il prezzo per bit si ridusse di tre ordini di grandezza in vent'anni in un'età d'oro della scalabilità delle DRAM. Questo ritmo non era sostenibile a lungo, e alla fine del ventesimo secolo e all'inizio del ventunesimo, la logica aveva superato significativamente la scalabilità della memoria. La recente scalabilità della logica è rallentata a un ritmo di miglioramenti di densità del trenta-quaranta percento ogni due anni. Ma questo è ancora buono rispetto alle DRAM che sono approssimativamente un ordine di grandezza più lente del loro picco, richiedendo ora dieci anni per un aumento di densità del doppio.

Questo rallentamento della scalabilità ha avuto effetti a catena nelle dinamiche dei prezzi delle DRAM. Mentre la memoria è stata tradizionalmente un'industria ciclica, la lenta scalabilità della densità ha significato una riduzione dei costi molto minore per ammortizzare gli aumenti di prezzo quando l'offerta è limitata. L'unico modo per aumentare l'offerta di DRAM è costruire nuove fabbriche. Le oscillazioni selvagge dei prezzi e gli alti investimenti in capitale significano che solo le aziende più grandi sopravvivono: più di venti produttori producevano DRAM a metà degli anni Novanta, con l'ottanta percento della quota di mercato distribuita tra i primi dieci. Ora i primi tre fornitori possiedono oltre il novantacinque percento del mercato.

Poiché le DRAM sono mercificate, i fornitori sono intrinsecamente molto più suscettibili alle fluttuazioni dei prezzi, a differenza della logica o dell'analogico, e devono competere principalmente sui prezzi grezzi dei loro prodotti quando il mercato è basso. La logica ha mantenuto la Legge di Moore solo con costi crescenti, le DRAM non hanno quel lusso. Il costo delle DRAM è semplice da misurare: dollari per gigabit. Rispetto ai periodi precedenti, gli ultimi dieci anni hanno visto una lenta diminuzione dei prezzi, solo un ordine di grandezza in un decennio quando prima ci voleva la metà di quel tempo.

## Le Sfide della Scalabilità Moderna

Da quando sono entrate nei nodi da dieci nanometri, la densità dei bit delle DRAM è stagnata. Anche l'aggiunta della litografia a ultravioletti estremi, nota come EUV, nei nodi 1z di Samsung e 1a di SK Hynix non ha aumentato significativamente la densità. Due sfide notevoli riguardano i condensatori e gli amplificatori di senso.

I condensatori sono difficili sotto molti aspetti. Innanzitutto, la modellazione è impegnativa poiché i fori devono essere strettamente impacchettati con un ottimo controllo della dimensione critica e della sovrapposizione, per contattare i transistor di accesso sottostanti ed evitare ponti o altri difetti. I condensatori hanno un rapporto di aspetto molto elevato e incidere un profilo di foro dritto e stretto è eccezionalmente difficile, ulteriormente complicato dalla necessità di una maschera dura più spessa per consentire un'incisione più profonda, poiché una maschera più spessa richiede un fotoresist più spesso che è più difficile da modellare.

Successivamente, devono essere depositati multipli strati privi di difetti di pochi nanometri di spessore sulle pareti lungo tutto il profilo del foro per formare il condensatore. Quasi ogni passaggio sforza i limiti della moderna tecnologia di elaborazione.

Gli amplificatori di senso sono una storia simile agli interconnessi logici. Una volta un ripensamento, ora sono di pari o addirittura maggiore difficoltà rispetto alle caratteristiche principali, ovvero i transistor logici e le celle di memoria. Sono compressi da più lati. La scalabilità dell'area deve essere eseguita per corrispondere alla riduzione delle bitline, con gli amplificatori di senso che diventano meno sensibili e più inclini a variazioni e perdite man mano che vengono resi più piccoli. Allo stesso tempo, i condensatori più piccoli immagazzinano meno carica, quindi il requisito di rilevamento per leggerli diventa più difficile.

Ci sono anche altre sfide, con il risultato che la scalabilità delle DRAM in modo economico è sempre più difficile utilizzando approcci tradizionali. La porta è aperta a nuove idee.

## Scalabilità a Breve Termine: Layout 4F² e Transistor a Canale Verticale

Nel breve termine, la scalabilità delle DRAM continuerà lungo la sua roadmap tradizionale. Cambiamenti più grandi e fondamentali all'architettura richiederanno anni per essere sviluppati e implementati. Nel frattempo, l'industria deve rispondere alla necessità di prestazioni migliori, anche se solo con miglioramenti marginali.

La roadmap a breve termine ha due innovazioni: il layout delle celle 4F² e i transistor a canale verticale, noti come VCT (Vertical Channel Transistor).

La notazione 4F² descrive l'area della cella di memoria in termini della dimensione minima della caratteristica F, simile alla metrica del track per l'altezza delle celle logiche standard. La dimensione minima della caratteristica è generalmente la larghezza della linea o dello spazio; nelle DRAM questa sarà la larghezza della wordline o della bitline. È un modo semplice per denotare la densità di un layout di celle e rende il confronto facile: una cella 4F² è solo i due terzi della dimensione di una cella 6F², offrendo un aumento teorico di densità del trenta percento senza scalare la dimensione minima della caratteristica. Si noti che il layout puro della cella non è l'unico limite alla scalabilità della densità, quindi i benefici reali sono probabilmente inferiori al caso ideale del trenta percento.

4F² è il limite teorico per una cella a bit singolo. Ricordiamo che la dimensione della caratteristica è la larghezza della linea o dello spazio, quindi un pattern linea più spazio avrà un pitch di 2F, non F, e quindi la dimensione minima possibile della cella è 4F² non solo F². Quindi, una volta raggiunta questa architettura, l'unica via per la scalabilità orizzontale è scalare F stesso, qualcosa che sta rapidamente diventando impraticabile, se non addirittura impossibile.

Le DRAM hanno utilizzato un layout 6F² dal 2007, con 8F² prima di allora. Un'eccezione notevole è CXMT, un fornitore cinese che ha utilizzato VCT e un layout 4F² nelle loro DRAM da diciotto nanometri dimostrate alla fine del 2023, progettate per aggirare le sanzioni. Poiché Samsung, SK Hynix e Micron sono stati in grado di scalare le celle, non sono stati costretti ad adottare queste architetture nello stesso modo in cui lo è stata CXMT. L'implicazione dell'adozione anticipata di CXMT è anche importante: è probabile che stiano avendo difficoltà a scalare F poiché hanno optato per il cambiamento più drastico nelle architetture di celle e transistor.

L'abilitatore chiave per le celle 4F² è il transistor a canale verticale. È necessario semplicemente perché il transistor deve scalare per adattarsi nella cella e entrambi i contatti, alla bitline e al condensatore, devono anche adattarsi in quell'ingombro, quindi una linea verticale. A queste scale diventa necessario costruire il transistor verticalmente invece che orizzontalmente, riducendo il suo ingombro fino a circa 1F, corrispondendo approssimativamente al condensatore sopra di esso, pur mantenendo una lunghezza di canale sufficiente affinché il transistor funzioni efficacemente.

Le attuali DRAM utilizzano canali orizzontali e source/drain con separazione orizzontale. Queste sono architetture mature e ben comprese. I VCT impilano sequenzialmente un source collegato alla bitline sotto di esso, un canale circondato dal gate e dalla wordline che controlla il gate, e un drain collegato al condensatore sopra. Ci sono compromessi nella fabbricazione dove alcuni passaggi diventano più facili e altri più difficili, ma nel complesso i VCT sono più difficili da produrre.

Il processo di Samsung è notevole per l'uso del bonding dei wafer, ovvero l'incollaggio di wafer. In un processo simile alla fornitura di alimentazione dal lato posteriore per la logica, i transistor di accesso alle celle vengono fabbricati con bitline formate sopra prima di capovolgere il wafer e incollarlo a un wafer di supporto, quindi la bitline è ora sepolta. Interessante notare che la base incollata non sembra richiedere un allineamento accurato con i VCT, sebbene la divulgazione non spieghi se la CMOS periferica sarà sul chip capovolto o nella base appena incollata. Il lato superiore viene assottigliato per esporre l'altra estremità dei transistor in modo che i condensatori di storage possano essere costruiti sopra di essi.

## Le Varianti Attuali di DRAM

Le DRAM sono disponibili in molte varietà, ciascuna ottimizzata per obiettivi diversi. Le varianti più recenti rilevanti sono DDR5, LPDDR5X, GDDR6X e HBM3/E. Le differenze tra loro risiedono quasi interamente nei circuiti periferici. Le celle di memoria stesse sono simili tra le varietà e i metodi di fabbricazione sono in generale simili per tutti i tipi.

La DDR5, dove DDR sta per Double Data Rate quinta generazione, offre la massima capacità di memoria poiché è confezionata in moduli di memoria dual in-line, noti come DIMM. La LPDDR5X, Low Power DDR5 con X che significa enhanced, ovvero migliorata, fornisce un funzionamento a basso consumo ma richiede distanze più brevi e connessioni a bassa capacitanza alla CPU che limitano la capacità, quindi viene utilizzata in telefoni cellulari e laptop dove il basso consumo è desiderabile e i vincoli di layout tollerabili.

Più recentemente abbiamo visto packaging a capacità maggiore per LPDDR utilizzato in alcuni acceleratori per l'intelligenza artificiale, nelle workstation professionali di Apple e nelle CPU feeder per AI come Grace. Questi nuovi usi sono guidati dalla ricerca di trasferimenti di dati efficienti dal punto di vista energetico e alta larghezza di banda.

Negli acceleratori, l'LPDDR è emersa come la migliore opzione per un secondo livello di memoria che fornisce capacità più economica a un livello inferiore e più lento rispetto alla costosa HBM. Non raggiunge le capacità massime e le caratteristiche di affidabilità, ma batte i DIMM DDR5 poiché consuma un ordine di grandezza meno energia per bit di throughput.

Il packaging LPDDR5X arriva fino a quattrocentottanta gigabyte disponibili sul processore Nvidia Grace, che è circa dieci volte il limite di capacità per le configurazioni GDDR, limitate dalle regole di layout del circuito stampato e del packaging dei chip necessari per soddisfare i requisiti di segnale nei sistemi di gioco consumer, e nella stessa gamma delle configurazioni server DDR medie. Una capacità DDR5 maggiore è possibile utilizzando R-DIMM di dimensioni superiori a centoventotto gigabyte, sebbene costosi a causa della complessità del packaging e dei chip Registers aggiuntivi, una sorta di chip buffer, sui DIMM.

L'LPDDR5X ha un grande vantaggio nel consumo energetico rispetto al DDR e nel costo rispetto all'HBM, ma l'energia per bit non può sfidare l'HBM e richiede molte lane, ovvero connessioni alla CPU, che affollano i layout delle schede a capacità maggiori. Ha anche una storia debole sulla correzione degli errori, nota come ECC (Error Correction Code), che diventa più importante a capacità maggiori poiché c'è una maggiore possibilità di errore. Per compensare, una certa capacità deve essere deviata per supportare ECC extra. Ad esempio, la CPU Grace ha cinquecentododici gigabyte di LPDDR5x per tray di calcolo ma sembra riservare trentadue gigabyte per funzionalità di affidabilità, lasciando quattrocentottanta gigabyte disponibili per l'uso.

Il prossimo standard LPDDR6 mostra pochi miglioramenti, mantenendo alti conteggi di lane per chip e aumenti di velocità relativamente moderati insieme a un supporto limitato per la correzione degli errori. L'LPDDR6 non fornirà un concorrente per l'HBM.

La GDDR6X, dove G sta per Graphics, si concentra sulle applicazioni grafiche, offrendo alta larghezza di banda a basso costo ma con latenza più alta e maggiore consumo energetico. Sebbene utile nelle GPU da gaming, è stata progettata con limiti di capacità a livello di scheda e livelli di potenza che limitano la dimensione delle applicazioni AI che possono utilizzarla.

Poi c'è l'HBM3E, High Bandwidth Memory terza generazione con una versione enhanced migliorata. Dà priorità alla larghezza di banda e all'efficienza energetica ma è molto costosa. Le due caratteristiche distintive dell'HBM sono la larghezza del bus molto più ampia e i die di memoria impilati verticalmente. I singoli die HBM hanno duecentocinquantasei bit ciascuno di input/output, sedici volte più dell'LPDDR che ha una larghezza del bus di soli sedici bit per chip. I die sono impilati verticalmente, tipicamente otto o più, con input/output raggruppati per ogni quattro die; in totale il package può fornire milleventiquattro bit di larghezza di banda. Nell'HBM4 questo raddoppierà a duemilaquarantotto bit. Per sfruttare al meglio l'HBM è meglio co-confezionarlo accanto al motore di calcolo per ridurre latenza ed energia per bit. Per espandere la capacità mantenendo una connessione breve al calcolo, devono essere aggiunti più die allo stack.

L'alto costo dell'HBM è principalmente guidato da questa necessità di impilamento dei die. In uno stack HBM tipico, otto o dodici die DRAM, con sedici e oltre sulla roadmap, sono impilati uno sopra l'altro, con alimentazione e segnale instradati da Through Silicon Via, abbreviati come TSV, in ogni die. I TSV sono collegamenti che passano direttamente attraverso il chip, che consentono la connessione tra chip. I TSV sono molto più densi, più performanti e più costosi dei vecchi metodi di wire-bonding, ovvero saldatura a filo, utilizzati per collegare chip impilati. Più di milleduecento fili di segnale devono essere instradati tramite TSV in uno stack HBM. Un'area significativa deve essere dedicata a loro, rendendo ogni die DRAM HBM il doppio delle dimensioni di un die DDR standard per la stessa capacità. Ciò significa anche requisiti di binning più elevati per le prestazioni elettriche e termiche del die DRAM.

Questa complessità riduce la resa. Ad esempio, gli errori di progettazione DRAM di Samsung e il loro uso di un nodo 1α in ritardo stanno contribuendo alle loro rese HBM scioccantemente scarse. Il packaging è l'altra sfida principale. Allineare correttamente otto o più die con migliaia di connessioni ciascuno è difficile e quindi costoso a causa di rese relativamente basse. Al momento questo è uno dei differenziatori chiave tra i fornitori di HBM, poiché SK Hynix può produrre con successo HBM3E con il loro packaging MR-MUF mentre Samsung fatica a ottenere resa dal loro prodotto. Micron ha una soluzione praticabile, ma deve scalare significativamente la produzione.

Nonostante gli alti costi e le sfide di resa, l'HBM3E è, per ora, il prodotto più prezioso e con il margine più alto che l'industria della memoria abbia mai avuto. Questo è principalmente perché per gli acceleratori AI di modelli grandi, nessun altro tipo di DRAM è un'alternativa praticabile. Mentre i margini probabilmente si eroderanno man mano che Samsung migliora la resa e Micron scala la produzione, l'appetito di memoria degli acceleratori AI continuerà a crescere, compensando in una certa misura il beneficio di questa nuova offerta.

In breve, l'alta larghezza di banda e la densità di packaging molto alta insieme alla migliore energia per bit e alla vera capacità ECC rendono l'HBM3E il chiaro vincitore, per ora, per gli acceleratori AI. Questo è il motivo per cui prodotti come l'H100 di Nvidia e l'MI300X di AMD lo utilizzano.

## La Roadmap dell'HBM e le Sfide Future

L'attuale soluzione HBM è costosa e sarà sempre più difficile da scalare. Come siamo finiti in questa situazione? L'HBM è una soluzione di packaging costruita attorno a idee DRAM legacy, ma confezionata con densità e adiacenza per cercare di risolvere i problemi di larghezza di banda e potenza per l'intelligenza artificiale e altre forme di calcolo ad alte prestazioni.

Tutte le GPU AI leader ora utilizzano HBM come memoria. I piani per il 2025 prevedono HBM3e a dodici strati con chip da trentadue gigabit per un totale di quarantotto gigabyte per stack, con velocità di dati fino a otto gigabit al secondo per filo. Nei server GPU le prime versioni di memoria unificata con una CPU di supporto sono state lanciate con l'MI300A di AMD e il Grace Hopper di Nvidia.

La CPU Grace ha LPDDR5X ad alta capacità, mentre la GPU ha HBM3 ad alta larghezza di banda. Tuttavia, la CPU e la GPU sono su package separati, collegati tramite NVLink-C2C a novecento gigabyte al secondo. Questo modello è più semplice da integrare ma più difficile sul lato software. La latenza della memoria collegata all'altro chip è molto più alta e potrebbe influenzare un numero significativo di workload. Come tale, la memoria non è del tutto uniforme e presenta le proprie sfide.

L'HBM4
